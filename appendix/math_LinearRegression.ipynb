{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B51XLWgIAAxt"
   },
   "source": [
    "# 線形回帰の最小二乗法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4PXpwLpoADE4"
   },
   "source": [
    "線形回帰モデルにおける二乗誤差の式は以下の通り  \n",
    "（式番号はITエンジニアのための機械学習理論入門に基づく）\n",
    "\n",
    "$$\n",
    "E_D = \\frac{1}{2} \\sum^N_{n=1}{ (\\sum^M_{m=0}{w_mx_n^m - t_n) ^2} } \\tag{2.4}\n",
    "$$\n",
    "\n",
    "$w_m$ の最適値を求めるため、$E_D$ を $w_m$ に関して偏微分すると、下記のようになる。  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_D}{\\partial w_m}  = \\sum^N_{n=1}{ (\\sum^M_{m'=0}{w_{m'}x_n^{m'} - t_n) x^m_n} } \\tag{2.7}\n",
    "$$\n",
    "\n",
    "$x^m_n$ はスカラー値であるため、$\\sum$ の内部に展開することができる。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_D}{\\partial w_m}  = \\sum^N_{n=1}{ (\\sum^M_{m'=0}{w_{m'}x_n^{m'} x^m_n - t_n x^m_n)} }\n",
    "$$\n",
    "\n",
    "$\\sum$ の中身が単純な加算・減算であれば、展開して計算を行っても結果が変わらない。つまり、 $\\sum{(A - B)} = \\sum{A} - \\sum{B}$ であるため、\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_D}{\\partial w_m}  = \\sum^N_{n=1}{ \\sum^M_{m'=0}{w_{m'}x_n^{m'} x^m_n - \\sum^N_{n=1}t_n x^m_n} }\n",
    "$$\n",
    "\n",
    "同様に、二重和 $\\sum^N_{n=1}\\sum^M_{m'=0}$は逆転させても結果が変わらないので、\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_D}{\\partial w_m}  = \\sum^M_{{m'}=0}{ \\sum^N_{n=1}{w_{m'}x_n^{m'} x^m_n - \\sum^N_{n=1}t_n x^m_n} }\n",
    "$$\n",
    "\n",
    "$w_{m'}$ は $n$ の値に依存せず一定であるため、 $\\sum^{N}_{n=1}$ の外に括りだすことができる。従って、以下のように表すことができる。  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_D}{\\partial w_m}  = \\sum^M_{{m'}=0}w_{m'}{ \\sum^N_{n=1}{x_n^{m'} x^m_n - \\sum^N_{n=1}t_n x^m_n} } \\tag{2.8}\n",
    "$$\n",
    "\n",
    "ここで、${\\bf w, t, \\phi_m, \\Phi}$ を以下のように定義する。\n",
    "\n",
    "$$\n",
    "{\\bf w} = \\begin{pmatrix}\n",
    "w_0 \\\\\n",
    "\\vdots\\\\\n",
    "w_M \\\\\n",
    "\\end{pmatrix},\n",
    "{\\bf t} = \\begin{pmatrix}\n",
    "t_0 \\\\\n",
    "\\vdots\\\\\n",
    "t_N \\\\\n",
    "\\end{pmatrix},\n",
    "{\\bf \\phi_m} = \n",
    "\\begin{pmatrix}\n",
    "x_1^m \\\\\n",
    "\\vdots\\\\\n",
    "x_N^m \\\\\n",
    "\\end{pmatrix},\n",
    "{\\bf \\Phi} = \n",
    "\\begin{pmatrix}\n",
    "x_1^0 & \\dots & x_1^M\\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "x_N^0 & \\dots & x_N^M \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "すると、以下のように表すことができる。\n",
    "\n",
    "$$\n",
    "\\sum^M_{{m'}=0}w_{m'} \\sum^N_{n=1}{x_n^{m'} x^m_n} = {\\bf w}^T {\\bf \\Phi}^T {\\bf \\phi_m} \\\\\n",
    "\\sum^N_{n=1}t_n x^m_n = {\\bf t}^T {\\bf \\phi_m}\n",
    "$$\n",
    "\n",
    "従って、式(2.8) は、下記のように表すことができる。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_D}{\\partial w_m}  = {\\bf w}^T {\\bf \\Phi}^T {\\bf \\phi_m} -  {\\bf t}^T {\\bf \\phi_m}\n",
    "$$\n",
    "\n",
    "これを全ての${\\bf w}=(w_0, \\dots, w_M)^T$ について考えると、下記のように表すことができる。\n",
    "\n",
    "$$\n",
    "\\nabla E_D({\\bf w})  = {\\bf w}^T {\\bf \\Phi}^T {\\bf \\Phi} -  {\\bf t}^T {\\bf \\Phi} \\tag{2.9}\n",
    "$$\n",
    "\n",
    "${\\bf w}$ の最適値は、$\\nabla E_D({\\bf w}) = 0$ となる点。つまり、(2.9) が0になる時の${\\bf w}$なので、\n",
    "\n",
    "$$\n",
    "{\\bf w} = {\\bf (\\Phi^T \\Phi)^{-1} \\Phi^T t}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "1_LinearRegression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
